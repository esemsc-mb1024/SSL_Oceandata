{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eac153f-4923-46a1-86bd-c2145d44c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/mb1024/home/myenv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torchvision.transforms as T\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from timm.models.vision_transformer import vit_base_patch16_224\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from lightly.models import utils\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from lightly.transforms import MAETransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1849e6-aea8-4557-8081-0c8240070e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be936a79-ff86-45a3-a4ae-b33df7874de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a221e13d-9994-4348-be67-22663a92cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68839b3-9455-46c6-83b1-82868c9f75e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-msssim in ./myenv/lib64/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: torch in ./myenv/lib64/python3.11/site-packages (from pytorch-msssim) (2.7.1)\n",
      "Requirement already satisfied: filelock in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./myenv/lib64/python3.11/site-packages (from torch->pytorch-msssim) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./myenv/lib64/python3.11/site-packages (from triton==3.3.1->torch->pytorch-msssim) (65.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib64/python3.11/site-packages (from sympy>=1.13.3->torch->pytorch-msssim) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib64/python3.11/site-packages (from jinja2->torch->pytorch-msssim) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5634edc8-8285-4b97-a1d0-7babba01af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from lightly.models.modules import MAEDecoderTIMM\n",
    "\n",
    "class GrayscaleDecoder(MAEDecoderTIMM):\n",
    "    def __init__(self, num_patches, patch_size, embed_dim, decoder_embed_dim,\n",
    "                 decoder_depth, decoder_num_heads, mlp_ratio,\n",
    "                 proj_drop_rate, attn_drop_rate):\n",
    "        \n",
    "        # ‚ö†Ô∏è Pass in_chans=1 for grayscale output\n",
    "        super().__init__(\n",
    "            num_patches=num_patches,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=1,  # üëà required for grayscale\n",
    "            embed_dim=embed_dim,\n",
    "            decoder_embed_dim=decoder_embed_dim,\n",
    "            decoder_depth=decoder_depth,\n",
    "            decoder_num_heads=decoder_num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            proj_drop_rate=proj_drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate\n",
    "        )\n",
    "\n",
    "        # Override the default decoder_pred with 1-channel output\n",
    "        self.predict = nn.Linear(decoder_embed_dim, patch_size * patch_size * 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decode(x)       # inherited from MAEDecoderTIMM\n",
    "        x = self.predict(x)      # shape: [B, num_masked_patches, patch_dim]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1aed58-473d-4e41-90af-9d67fe28e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(self, vit):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_dim = 512\n",
    "        self.mask_ratio = 0.1\n",
    "        self.patch_size = vit.patch_embed.patch_size[0]\n",
    "\n",
    "        self.backbone = MaskedVisionTransformerTIMM(vit=vit)\n",
    "        self.sequence_length = self.backbone.sequence_length\n",
    "        self.decoder = GrayscaleDecoder(\n",
    "                num_patches=vit.patch_embed.num_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                embed_dim=vit.embed_dim,\n",
    "                decoder_embed_dim=decoder_dim,\n",
    "                decoder_depth=1,\n",
    "                decoder_num_heads=16,\n",
    "                mlp_ratio=4.0,\n",
    "                proj_drop_rate=0.0,\n",
    "                attn_drop_rate=0.0,\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward_encoder(self, images, idx_keep=None):\n",
    "        return self.backbone.encode(images=images, idx_keep=idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "        # build decoder input\n",
    "        batch_size = x_encoded.shape[0]\n",
    "        x_decode = self.decoder.embed(x_encoded)\n",
    "        x_masked = utils.repeat_token(\n",
    "            self.decoder.mask_token, (batch_size, self.sequence_length)\n",
    "        )\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode.type_as(x_masked))\n",
    "\n",
    "        # decoder forward pass\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "        # predict pixel values for masked tokens\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "        return x_pred\n",
    "\n",
    "    def forward(self, images, return_full_reconstruction=False):\n",
    "        batch_size = images.shape[0]\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=images.device,\n",
    "        )\n",
    "    \n",
    "        x_encoded = self.forward_encoder(images=images, idx_keep=idx_keep)\n",
    "        x_pred = self.forward_decoder(\n",
    "            x_encoded=x_encoded, idx_keep=idx_keep, idx_mask=idx_mask\n",
    "        )\n",
    "    \n",
    "        # Patchify original images\n",
    "        patches = utils.patchify(images, self.patch_size)\n",
    "    \n",
    "        # Target: ground truth patches at masked positions\n",
    "        target = utils.get_at_index(patches, idx_mask - 1)\n",
    "    \n",
    "        if return_full_reconstruction:\n",
    "            # Reconstruct full patch sequence\n",
    "            recon = patches.clone()  # [B, num_patches, patch_dim]\n",
    "            recon = utils.set_at_index(recon, idx_mask - 1, x_pred)\n",
    "    \n",
    "            return recon, patches  # full recon and original for unpatchify\n",
    "    \n",
    "        return x_pred, target\n",
    "\n",
    "\n",
    "# 1. Create the ViT model\n",
    "vit = vit_base_patch32_224(pretrained=False)  # or pretrained=True if fine-tuning\n",
    "\n",
    "# 2. Modify the patch embedding layer to accept 1-channel input\n",
    "old_proj = vit.patch_embed.proj\n",
    "vit.patch_embed.proj = nn.Conv2d(\n",
    "    in_channels=1,  # <--- key change\n",
    "    out_channels=old_proj.out_channels,\n",
    "    kernel_size=old_proj.kernel_size,\n",
    "    stride=old_proj.stride,\n",
    "    padding=old_proj.padding,\n",
    "    bias=old_proj.bias is not None\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 3. Now pass the modified ViT into your MAE model\n",
    "model = MAE(vit).to(device)\n",
    "\n",
    "\n",
    "\n",
    "transform = MAETransform()\n",
    "# we ignore object detection annotations by setting target_transform to return 0\n",
    "\n",
    "\n",
    "def target_transform(t):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0fdfcf0-8d7b-47bb-977a-f532c8f8e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stack = np.memmap(\"clean_combined_stack.dat\", dtype=np.float32, mode='r', shape=(49402, 204, 216))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13239121-cf4e-42d4-839c-ba25586e0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the memmap file (already done)\n",
    "final_stack = np.memmap(\"clean_combined_stack.dat\", dtype=np.float32, mode='r', shape=(49402, 204, 216))\n",
    "\n",
    "# Randomly choose 1000 unique indices\n",
    "sample_indices = np.random.choice(final_stack.shape[0], size=1000, replace=False)\n",
    "\n",
    "# Create a sample array by indexing (this will load only the selected slices)\n",
    "sample_stack = final_stack[sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "185402b5-c261-457d-8876-b43842bf044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UnlabelledNumpyDataset(Dataset):\n",
    "    def __init__(self, array_stack, transform=None):\n",
    "        self.data = array_stack  # shape: [N, H, W]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        arr = self.data[idx].astype(np.float32)\n",
    "\n",
    "        if np.std(arr) == 0:\n",
    "            arr += np.random.normal(0, 1e-6, size=arr.shape)\n",
    "\n",
    "        arr_norm = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
    "\n",
    "        img = torch.from_numpy(arr_norm).unsqueeze(0)  # [1, H, W]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Handle single or multi-view transform\n",
    "        if isinstance(img, list):\n",
    "            return img, img[0].clone()  # views, original\n",
    "        else:\n",
    "            return [img], img.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c344a99c-d88c-457e-b5d7-7159e5d2ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResizeTensor:\n",
    "    def __init__(self, size=(224, 224)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return F.interpolate(x.unsqueeze(0), size=self.size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "transform = ResizeTensor((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ce416c-1b27-4271-8779-84953e1ad4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UnlabelledNumpyDataset(final_stack, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502efcb-9306-46e2-9c77-6f1d2ff3b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a batch\n",
    "sample = dataset[0]  # First item\n",
    "views, original = sample\n",
    "\n",
    "print(f\"Type of views: {type(views)}\")\n",
    "print(f\"Number of views: {len(views)}\")\n",
    "print(f\"Shape of first view: {views[0].shape}\")\n",
    "print(f\"Shape of original: {original.shape}\")\n",
    "\n",
    "for views, original in dataloader:\n",
    "    print(views[0].shape)  # Should be [32, 1, 224, 224]\n",
    "    print(original.shape)  # Should also be [32, 1, 224, 224]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5172a129-8c27-48d7-a579-d784c65ab1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_msssim import ssim  # make sure you have this installed\n",
    "\n",
    "class SSIM_MSE_Loss(nn.Module):\n",
    "    def __init__(self, patch_size=model.patch_size, weight_ssim=0.85):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.weight_ssim = weight_ssim\n",
    "\n",
    "    def forward(self, pred_patches, target_patches):\n",
    "        \"\"\"\n",
    "        pred_patches: [B, num_masked, patch_dim]\n",
    "        target_patches: [B, num_masked, patch_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpatchify to full images made of only masked patches\n",
    "        # Treat each [B, num_masked, patch_dim] as a mini batch of \"flattened patches\"\n",
    "        # Reshape to fake image batch: [B * num_masked, 1, patch_size, patch_size]\n",
    "        B, N, D = pred_patches.shape\n",
    "        recon = pred_patches.view(-1, 1, self.patch_size, self.patch_size)\n",
    "        target = target_patches.view(-1, 1, self.patch_size, self.patch_size)\n",
    "\n",
    "        # SSIM expects values in [0, 1]\n",
    "        recon = recon.clamp(0, 1)\n",
    "        target = target.clamp(0, 1)\n",
    "\n",
    "        ssim_loss = 1 - ssim(recon, target, data_range=1.0, size_average=True)\n",
    "        mse_loss = F.mse_loss(recon, target)\n",
    "\n",
    "        # Combine losses\n",
    "        return self.weight_ssim * ssim_loss + (1 - self.weight_ssim) * mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed0523-3a34-47ec-96a2-eb0bcffe75f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of views: <class 'list'>\n",
      "Number of views: 1\n",
      "Shape of first view: torch.Size([1, 224, 224])\n",
      "Shape of original: torch.Size([1, 224, 224])\n",
      "torch.Size([32, 1, 224, 224])\n",
      "torch.Size([32, 1, 224, 224])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87b345-4897-4b76-90d6-de79af0906b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n"
     ]
    }
   ],
   "source": [
    "criterion = SSIM_MSE_Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-4)\n",
    "\n",
    "print(\"Starting Training\")\n",
    "for epoch in range(25):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        views, originals = batch  # views is a list of image views\n",
    "\n",
    "        # If you're using only one view\n",
    "        images = views[0].to(device)  # or batch[0].to(device)\n",
    "        if images.ndim == 3:\n",
    "            images = images.unsqueeze(1)  # add channel if missing\n",
    "\n",
    "        #print(\"Batch shape:\", images.shape)  # Debug\n",
    "\n",
    "        predictions, targets = model(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469783d4-5eda-4791-aaf3-e697ab07ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    # Get the first view and move it to the device\n",
    "    images = batch[0][0].to(device)\n",
    "\n",
    "    recon_patches, original_patches = model(images, return_full_reconstruction=True)\n",
    "\n",
    "    patch_size = model.patch_size\n",
    "    recon_images = utils.unpatchify(recon_patches, patch_size, channels=1).clamp(0, 1).cpu()\n",
    "    orig_images = utils.unpatchify(original_patches, patch_size, channels=1).clamp(0, 1).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727bc00-8be4-4342-b781-c9a26cadde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_reconstructions(original, reconstructed, num_images=5):\n",
    "    for i in range(num_images):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "        # Convert from [C, H, W] to [H, W] using only the first channel\n",
    "        img_orig = original[i][0].detach().cpu().numpy()  # shape: [H, W]\n",
    "        img_recon = reconstructed[i][0].detach().cpu().numpy()\n",
    "\n",
    "        axs[0].imshow(img_orig, cmap='gray', vmin=0, vmax=1)\n",
    "        axs[1].imshow(img_recon, cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "        axs[0].set_title(\"Original\")\n",
    "        axs[1].set_title(\"Reconstruction\")\n",
    "\n",
    "        for ax in axs:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "show_reconstructions(orig_images, recon_images, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca95ca-26f3-4513-a359-8cc828739fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (orig_images - recon_images).abs()\n",
    "show_reconstructions(diff, diff, num_images=5)  # Just for visualizing error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bae5f-6555-4882-8dc0-4d6069553917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_difference_maps(original, reconstructed, num_images=5):\n",
    "    for i in range(num_images):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "        orig = original[i].squeeze().cpu().numpy()\n",
    "        recon = reconstructed[i].squeeze().cpu().numpy()\n",
    "        diff = abs(orig - recon)\n",
    "\n",
    "        axs[0].imshow(orig, cmap='gray', vmin=0, vmax=1)\n",
    "        axs[0].set_title(\"Original\")\n",
    "\n",
    "        axs[1].imshow(recon, cmap='gray', vmin=0, vmax=1)\n",
    "        axs[1].set_title(\"Reconstruction\")\n",
    "\n",
    "        axs[2].imshow(diff, cmap='hot', vmin=0, vmax=1)\n",
    "        axs[2].set_title(\"Difference\")\n",
    "\n",
    "        for ax in axs:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "show_difference_maps(orig_images, recon_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc858d-d26f-438c-bb97-57774d1df035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lightly.models.utils import random_token_mask, patchify, get_at_index\n",
    "\n",
    "def print_first_image_masked_patch_diffs(model, dataloader, threshold=0.05):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1) load one batch\n",
    "        batch = next(iter(dataloader))\n",
    "        images = batch[0][0].to(device)   # shape [B, 1, H, W]\n",
    "\n",
    "        B = images.shape[0]\n",
    "        N = model.sequence_length\n",
    "\n",
    "        # 2) re-compute the mask exactly as in forward()\n",
    "        idx_keep, idx_mask = random_token_mask(\n",
    "            size=(B, N),\n",
    "            mask_ratio=model.mask_ratio,\n",
    "            device=images.device\n",
    "        )\n",
    "\n",
    "        # 3) run encoder + decoder (no full-forward)\n",
    "        x_enc = model.forward_encoder(images=images, idx_keep=idx_keep)\n",
    "        x_pred = model.forward_decoder(x_encoded=x_enc,\n",
    "                                       idx_keep=idx_keep,\n",
    "                                       idx_mask=idx_mask)\n",
    "        # x_pred: [B, N_masked, patch_dim]\n",
    "\n",
    "        # 4) build ground-truth patches from the original images\n",
    "        patches = patchify(images, model.patch_size)                # [B, N, patch_dim]\n",
    "        target = get_at_index(patches, idx_mask - 1)               # [B, N_masked, patch_dim]\n",
    "\n",
    "        # 5) print only the masked patches of the first image\n",
    "        image_idx = 0\n",
    "        for i, patch_idx in enumerate(idx_mask[image_idx]):\n",
    "            orig_patch = target[image_idx, i].view(model.patch_size, model.patch_size).cpu()\n",
    "            recon_patch = x_pred[image_idx, i].view(model.patch_size, model.patch_size).cpu()\n",
    "            diff_patch = (orig_patch - recon_patch).abs()\n",
    "\n",
    "            print(f\"\\nMasked Patch {i} (orig idx {patch_idx.item()}):\")\n",
    "            print(\"Original:\\n\", orig_patch.numpy())\n",
    "            print(\"Reconstruction:\\n\", recon_patch.numpy())\n",
    "            print(\"Difference:\\n\", diff_patch.numpy())\n",
    "            if threshold is not None:\n",
    "                sig = (diff_patch > threshold).numpy()\n",
    "                print(\"Significant (>%.2f)?:\\n\" % threshold, sig)\n",
    "\n",
    "        # done\n",
    "\n",
    "print_first_image_masked_patch_diffs(model, dataloader, threshold=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7d8a6-c9ab-4b0d-8075-81defb6b4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image min/max/std:\", orig_images[0].min(), orig_images[0].max(), orig_images[0].std())\n",
    "print(\"Max pixel difference:\", (orig_images[0] - recon_images[0]).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7f204865-dc9c-4830-ab23-30de90bf6aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Patch:\n",
      " [[0.20958407 0.20273536 0.19196671 ... 0.14722423 0.14722423 0.14722423]\n",
      " [0.23289695 0.22541648 0.22532508 ... 0.14722423 0.14722423 0.14722423]\n",
      " [0.23698346 0.2464652  0.25150046 ... 0.14722423 0.14722423 0.14722423]\n",
      " ...\n",
      " [0.36379367 0.33539593 0.26811457 ... 0.14722423 0.14722423 0.14722423]\n",
      " [0.30929333 0.28119618 0.23699147 ... 0.14722423 0.14722423 0.14722423]\n",
      " [0.2816503  0.27349547 0.25570482 ... 0.14722423 0.14722423 0.14722423]]\n",
      "Reconstructed Patch:\n",
      " [[0.0450802  0.12803799 0.09713861 ... 0.11671096 0.10220446 0.09234627]\n",
      " [0.12994742 0.07568821 0.08414418 ... 0.07583725 0.08153497 0.06298169]\n",
      " [0.12146682 0.06339622 0.03480439 ... 0.10631645 0.11968581 0.06660317]\n",
      " ...\n",
      " [0.09346808 0.13445729 0.11700937 ... 0.08817811 0.13836107 0.13829228]\n",
      " [0.07724376 0.07821012 0.10021586 ... 0.09604028 0.06926462 0.11693501]\n",
      " [0.13588268 0.10949293 0.06533957 ... 0.09922108 0.12187363 0.07907206]]\n",
      "Difference:\n",
      " [[0.16450387 0.07469738 0.0948281  ... 0.03051327 0.04501978 0.05487797]\n",
      " [0.10294953 0.14972827 0.1411809  ... 0.07138698 0.06568926 0.08424254]\n",
      " [0.11551665 0.18306899 0.21669607 ... 0.04090779 0.02753842 0.08062106]\n",
      " ...\n",
      " [0.2703256  0.20093864 0.1511052  ... 0.05904612 0.00886317 0.00893195]\n",
      " [0.23204957 0.20298606 0.13677561 ... 0.05118395 0.07795961 0.03028922]\n",
      " [0.14576763 0.16400254 0.19036525 ... 0.04800315 0.0253506  0.06815217]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "patch_size = model.patch_size\n",
    "\n",
    "# Get the patches\n",
    "orig_patch = original_patches[0, 20].view(patch_size, patch_size).cpu()\n",
    "recon_patch = recon_patches[0, 20].view(patch_size, patch_size).cpu()\n",
    "diff_patch = (orig_patch - recon_patch).abs()  # absolute difference\n",
    "\n",
    "# Print the pixel values\n",
    "print(\"Original Patch:\\n\", orig_patch.numpy())\n",
    "print(\"Reconstructed Patch:\\n\", recon_patch.numpy())\n",
    "print(\"Difference:\\n\", diff_patch.numpy())\n",
    "\n",
    "# Visualize\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "axs[0].imshow(orig_patch.numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "axs[0].set_title(\"Original Patch\")\n",
    "\n",
    "axs[1].imshow(recon_patch.numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "axs[1].set_title(\"Reconstructed Patch\")\n",
    "\n",
    "axs[2].imshow(diff_patch.numpy(), cmap='hot')\n",
    "axs[2].set_title(\"Absolute Difference\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d92f2294-3567-40bf-ac20-2118f1662380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for first image: 0.011371727101504803\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Get first image predictions and targets\n",
    "pred_0 = predictions[0]  # shape: [num_masked_patches, patch_dim]\n",
    "target_0 = targets[0]    # same shape\n",
    "\n",
    "# Compute MSE loss per element (no reduction yet)\n",
    "mse_per_element = F.mse_loss(pred_0, target_0, reduction=\"none\")\n",
    "\n",
    "# Average over patch_dim, then over all patches (standard MAE loss)\n",
    "loss_first_image = mse_per_element.mean()\n",
    "\n",
    "print(\"Loss for first image:\", loss_first_image.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
